import os
from typing import Annotated, List, Dict, Any, Optional, Literal
from dotenv import load_dotenv

# LangChain & LangGraph Imports
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.tools import tool
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel
from langchain_classic.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langgraph.graph import StateGraph, END, START
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from typing import TypedDict
from langchain_core.documents import Document

# 1. í™˜ê²½ ì„¤ì • ë° DB ë¡œë“œ
load_dotenv()

# ì„ë² ë”© ëª¨ë¸ ì„¤ì •
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Chroma DB ë¡œë“œ
try:
    vectorstore = Chroma(
        collection_name="shelter_and_disaster_guidelines",
        embedding_function=embeddings,
        persist_directory="./chroma_db"
    )
except Exception as e:
    print(f"âŒ Chroma DB ë¡œë“œ ì‹¤íŒ¨: {e}")
    raise

# 2. í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ êµ¬ì„±

# 2-1. Semantic (Vector) Retriever
shelter_vector_retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 5,
        "filter": {"type": "shelter"}
    }
)

guideline_vector_retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 3,
        "filter": {"type": "disaster_guideline"}
    }
)

# 2-2. BM25 (Keyword) Retriever êµ¬ì„±
def create_bm25_retriever(doc_type: str) -> BM25Retriever:
    """BM25 í‚¤ì›Œë“œ ê¸°ë°˜ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±"""
    try:
        all_docs = vectorstore.get(where={"type": doc_type})
        
        if not all_docs or 'documents' not in all_docs:
            print(f"âš ï¸ {doc_type} ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        documents = []
        for i, text in enumerate(all_docs['documents']):
            metadata = all_docs['metadatas'][i] if 'metadatas' in all_docs else {}
            documents.append(Document(page_content=text, metadata=metadata))
        
        bm25_retriever = BM25Retriever.from_documents(documents)
        bm25_retriever.k = 5
        
        return bm25_retriever
    except Exception as e:
        print(f"âš ï¸ BM25 Retriever ìƒì„± ì‹¤íŒ¨: {e}")
        return None

shelter_bm25_retriever = create_bm25_retriever("shelter")
guideline_bm25_retriever = create_bm25_retriever("disaster_guideline")

# 2-3. Ensemble (Hybrid) Retriever
shelter_hybrid_retriever = None
if shelter_bm25_retriever:
    shelter_hybrid_retriever = EnsembleRetriever(
        retrievers=[shelter_vector_retriever, shelter_bm25_retriever],
        weights=[0.6, 0.4]
    )
else:
    shelter_hybrid_retriever = shelter_vector_retriever

guideline_hybrid_retriever = None
if guideline_bm25_retriever:
    guideline_hybrid_retriever = EnsembleRetriever(
        retrievers=[guideline_vector_retriever, guideline_bm25_retriever],
        weights=[0.7, 0.3]
    )
else:
    guideline_hybrid_retriever = guideline_vector_retriever

# 3. LCEL ë°©ì‹ì˜ RAG Chain

# 3-1. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
RAG_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ ì¬ë‚œ ì•ˆì „ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì•„ë˜ ì°¸ê³  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— **ì •í™•í•˜ê²Œ** ë‹µë³€í•˜ì„¸ìš”.

ë‹µë³€ ì‹œ ì£¼ì˜ì‚¬í•­:
- ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”
- ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
- ì¤‘ìš”í•œ ì •ë³´ëŠ” **ë³¼ë“œì²´**ë¡œ ê°•ì¡°í•˜ì„¸ìš”
"""),
    ("human", """ì°¸ê³  ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {question}""")
])

# 3-2. Document í¬ë§·íŒ… í•¨ìˆ˜
def format_docs(docs: List[Document]) -> str:
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í¬ë§·íŒ…"""
    if not docs:
        return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    formatted = []
    for i, doc in enumerate(docs, 1):
        formatted.append(f"[ë¬¸ì„œ {i}]\n{doc.page_content}")
    
    return "\n\n".join(formatted)

# 3-3. LCEL RAG Chain ìƒì„±
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# ëŒ€í”¼ì†Œ ê²€ìƒ‰ ì²´ì¸
shelter_rag_chain = (
    {
        "context": shelter_hybrid_retriever | RunnableLambda(format_docs),
        "question": RunnablePassthrough()
    }
    | RAG_PROMPT
    | llm
    | StrOutputParser()
)

# í–‰ë™ìš”ë ¹ ê²€ìƒ‰ ì²´ì¸
guideline_rag_chain = (
    {
        "context": guideline_hybrid_retriever | RunnableLambda(format_docs),
        "question": RunnablePassthrough()
    }
    | RAG_PROMPT
    | llm
    | StrOutputParser()
)

# 4. LCEL ë°©ì‹ì˜ ì§ˆë¬¸ ë¶„ë¥˜ê¸°

CLASSIFICATION_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ì„¸ìš”.

ë¶„ë¥˜ ê¸°ì¤€:
1. simple_shelter: íŠ¹ì • ì§€ì—­ì˜ ëŒ€í”¼ì†Œ ê²€ìƒ‰
2. simple_guideline: ì¬ë‚œ í–‰ë™ìš”ë ¹ ì§ˆë¬¸
3. statistics: í†µê³„/ì§‘ê³„ ì§ˆë¬¸
4. complex: ë³µì¡í•œ ì§ˆë¬¸

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:
type: [simple_shelter|simple_guideline|statistics|complex]
confidence: [0.0-1.0]"""),
    ("human", "{query}")
])

def parse_classification(response: str) -> Dict[str, Any]:
    """ë¶„ë¥˜ ê²°ê³¼ íŒŒì‹±"""
    lines = response.strip().split('\n')
    result = {"type": "complex", "confidence": 0.5}
    
    for line in lines:
        if line.startswith("type:"):
            result["type"] = line.split(":", 1)[1].strip()
        elif line.startswith("confidence:"):
            try:
                result["confidence"] = float(line.split(":", 1)[1].strip())
            except:
                pass
    
    return result

# ë¶„ë¥˜ ì²´ì¸
classification_chain = (
    CLASSIFICATION_PROMPT
    | llm
    | StrOutputParser()
    | RunnableLambda(parse_classification)
)

# 4-2. ì§ˆë¬¸ ì¬ì •ì˜ ì²´ì¸ ì¶”ê°€

QUERY_REWRITE_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìì—°ì–´ ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ë³€í™˜í•˜ì„¸ìš”.

**ë³€í™˜ ê·œì¹™**:
1. êµ¬ì–´ì²´ ì œê±°: "ì¢€", "ì•Œë ¤ì¤˜", "ìˆì–´?" ë“± ì œê±°
2. í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ: ê²€ìƒ‰ì— ì¤‘ìš”í•œ ë‹¨ì–´ë§Œ ë‚¨ê¹€
3. ê²€ìƒ‰ ì˜ë„ ëª…í™•í™”: ëŒ€í”¼ì†Œ/í–‰ë™ìš”ë ¹/í†µê³„ ë“± ëª…ì‹œ
4. ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì •ë³´ ì¶”ê°€: ì§€í•˜/ì§€ìƒ, ì§€ì—­ëª…, ìˆ˜ìš©ì¸ì› ë“±

**ì˜ˆì‹œ**:
- "ê°•ë‚¨êµ¬ì— ìˆëŠ” ëŒ€í”¼ì†Œ ì¢€ ì•Œë ¤ì¤˜" â†’ "ê°•ë‚¨êµ¬ ëŒ€í”¼ì†Œ"
- "ì§€í•˜ì— ìœ„ì¹˜í•œ ëŒ€í”¼ì†ŒëŠ” ëª‡ ê°œì•¼?" â†’ "ì§€í•˜ ìœ„ì¹˜ ëŒ€í”¼ì†Œ ê°œìˆ˜ í†µê³„"
- "ì§€ì§„ ë‚¬ì„ ë•Œ ì–´ë–»ê²Œ í•´?" â†’ "ì§€ì§„ ë°œìƒ ì‹œ í–‰ë™ìš”ë ¹"

ì›ë³¸ ì§ˆë¬¸ë§Œ ë³€í™˜í•˜ê³ , ì¶”ê°€ ì„¤ëª… ì—†ì´ ë³€í™˜ëœ ì¿¼ë¦¬ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""),
    ("human", "{original_query}")
])

# ì§ˆë¬¸ ì¬ì •ì˜ ì²´ì¸
query_rewrite_chain = (
    QUERY_REWRITE_PROMPT
    | llm
    | StrOutputParser()
)

# 5. ë„êµ¬(Tools) ì •ì˜ - Query Rewriting ì ìš©

@tool
def search_shelter(query: str) -> str:
    """
    ì£¼ì†Œ, ì§€ì—­ëª…, ì‹œì„¤ëª… ë“±ì„ ì…ë ¥ë°›ì•„ 'ë¯¼ë°©ìœ„ ëŒ€í”¼ì†Œ' ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(ë²¡í„° + í‚¤ì›Œë“œ)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    try:
        # ì§ˆë¬¸ ì¬ì •ì˜
        rewritten_query = query_rewrite_chain.invoke({"original_query": query})
        print(f"ğŸ”„ ì›ë³¸: {query}")
        print(f"ğŸ” ì¬ì •ì˜: {rewritten_query}")
        
        # ì¬ì •ì˜ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
        docs = shelter_hybrid_retriever.invoke(rewritten_query)
        
        if not docs:
            return "ê²€ìƒ‰ëœ ëŒ€í”¼ì†Œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."

        seen = set()
        results = []
        for doc in docs:
            facility_name = doc.metadata.get('facility_name', 'ì•Œ ìˆ˜ ì—†ìŒ')
            if facility_name in seen:
                continue
            seen.add(facility_name)
            
            info = (
                f"ğŸ“ ì‹œì„¤ëª…: {facility_name}\n"
                f"   - ì£¼ì†Œ: {doc.metadata.get('address', 'ì£¼ì†Œ ì •ë³´ ì—†ìŒ')}\n"
                f"   - ìœ„ì¹˜: {doc.metadata.get('shelter_type', '')}\n"
                f"   - ìˆ˜ìš©ì¸ì›: {doc.metadata.get('capacity', 0)}ëª…\n"
                f"   - ì‹œì„¤êµ¬ë¶„: {doc.metadata.get('facility_type', '')}"
            )
            results.append(info)
            
            if len(results) >= 4:
                break
        
        return "\n---\n".join(results)
    
    except Exception as e:
        return f"ëŒ€í”¼ì†Œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"

@tool
def search_guideline(query: str) -> str:
    """
    ì¬ë‚œ í–‰ë™ ìš”ë ¹ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(ë²¡í„° + í‚¤ì›Œë“œ)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    try:
        # ì§ˆë¬¸ ì¬ì •ì˜
        rewritten_query = query_rewrite_chain.invoke({"original_query": query})
        print(f"ğŸ”„ ì›ë³¸: {query}")
        print(f"ğŸ” ì¬ì •ì˜: {rewritten_query}")
        
        docs = guideline_hybrid_retriever.invoke(rewritten_query)
        
        if not docs:
            return "ê´€ë ¨ëœ í–‰ë™ ìš”ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        results = []
        for doc in docs:
            meta = doc.metadata
            header = f"ğŸš¨ [{meta.get('category', 'ì¬ë‚œ')}] {meta.get('situation', 'ìƒí™©')} - {meta.get('title', '')}"
            content = doc.page_content
            results.append(f"{header}\n{content}")
            
        return "\n===\n".join(results)
    
    except Exception as e:
        return f"í–‰ë™ìš”ë ¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"

@tool
def count_shelters_by_capacity(min_capacity: int) -> str:
    """íŠ¹ì • ìˆ˜ìš©ì¸ì› ì´ìƒì˜ ëŒ€í”¼ì†Œ ê°œìˆ˜ë¥¼ ì§‘ê³„í•©ë‹ˆë‹¤."""
    try:
        all_shelters = vectorstore.get(where={"type": "shelter"})
        
        if not all_shelters or 'metadatas' not in all_shelters:
            return "ëŒ€í”¼ì†Œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        count = 0
        filtered_shelters = []
        
        for metadata in all_shelters['metadatas']:
            capacity = metadata.get('capacity', 0)
            try:
                capacity_num = int(capacity)
                if capacity_num >= min_capacity:
                    count += 1
                    filtered_shelters.append({
                        'name': metadata.get('facility_name', 'ì•Œ ìˆ˜ ì—†ìŒ'),
                        'capacity': capacity_num,
                        'address': metadata.get('address', 'ì£¼ì†Œ ì •ë³´ ì—†ìŒ')
                    })
            except (ValueError, TypeError):
                continue
        
        if count == 0:
            return f"ìˆ˜ìš©ì¸ì› {min_capacity:,}ëª… ì´ìƒì˜ ëŒ€í”¼ì†Œê°€ ì—†ìŠµë‹ˆë‹¤."
        
        filtered_shelters.sort(key=lambda x: x['capacity'], reverse=True)
        
        result = f"ğŸ“Š **ì „êµ­ ìˆ˜ìš©ì¸ì› {min_capacity:,}ëª… ì´ìƒ ëŒ€í”¼ì†Œ: ì´ {count}ê°œ**\n\n"
        result += "**[ìƒìœ„ 5ê°œ ëŒ€í”¼ì†Œ]**\n"
        for i, shelter in enumerate(filtered_shelters[:5], 1):
            result += (
                f"{i}. **{shelter['name']}**\n"
                f"   - ìˆ˜ìš©ì¸ì›: {shelter['capacity']:,}ëª…\n"
                f"   - ì£¼ì†Œ: {shelter['address']}\n"
            )
        
        if count > 5:
            result += f"\n*(ì™¸ {count - 5}ê°œ ë” ìˆìŒ)*"
        
        return result
    
    except Exception as e:
        return f"ëŒ€í”¼ì†Œ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}"

@tool
def get_shelter_statistics() -> str:
    """ì „êµ­ ëŒ€í”¼ì†Œì˜ ì „ì²´ í†µê³„ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."""
    try:
        all_shelters = vectorstore.get(where={"type": "shelter"})
        
        if not all_shelters or 'metadatas' not in all_shelters:
            return "ëŒ€í”¼ì†Œ í†µê³„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        metadatas = all_shelters['metadatas']
        total_count = len(metadatas)
        
        capacities = []
        regions = {}
        
        for meta in metadatas:
            try:
                cap = int(meta.get('capacity', 0))
                capacities.append(cap)
            except (ValueError, TypeError):
                pass
            
            address = meta.get('address', '')
            if address:
                region = address.split()[0]
                regions[region] = regions.get(region, 0) + 1
        
        if not capacities:
            return "ìˆ˜ìš©ì¸ì› ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        avg_capacity = sum(capacities) / len(capacities)
        max_capacity = max(capacities)
        min_capacity = min(capacities)
        
        top_regions = sorted(regions.items(), key=lambda x: x[1], reverse=True)[:5]
        
        result = f"""
ğŸ“Š **ì „êµ­ ëŒ€í”¼ì†Œ í†µê³„**

**ê¸°ë³¸ ì •ë³´**
- ì´ ëŒ€í”¼ì†Œ ìˆ˜: {total_count:,}ê°œ
- í‰ê·  ìˆ˜ìš©ì¸ì›: {avg_capacity:,.0f}ëª…
- ìµœëŒ€ ìˆ˜ìš©ì¸ì›: {max_capacity:,}ëª…
- ìµœì†Œ ìˆ˜ìš©ì¸ì›: {min_capacity:,}ëª…

**ì§€ì—­ë³„ ë¶„í¬ (ìƒìœ„ 5ê°œ)**
"""
        for i, (region, count) in enumerate(top_regions, 1):
            result += f"{i}. {region}: {count:,}ê°œ\n"
        
        return result.strip()
    
    except Exception as e:
        return f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}"

tools = [search_shelter, search_guideline, count_shelters_by_capacity, get_shelter_statistics]

# 6. í•˜ì´ë¸Œë¦¬ë“œ ê·¸ë˜í”„ ìƒíƒœ ì •ì˜

class HybridAgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    query_type: Optional[str]
    use_rag: bool
    context: Optional[str]

# 7. ë…¸ë“œ í•¨ìˆ˜ë“¤ (LCEL ì‚¬ìš©)

llm_with_tools = llm.bind_tools(tools)

def classifier_node(state: HybridAgentState):
    """ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” ë…¸ë“œ (LCEL ì‚¬ìš©)"""
    last_message = state["messages"][-1]
    query = last_message.content if isinstance(last_message, HumanMessage) else ""
    
    # LCEL ì²´ì¸ ì‹¤í–‰
    classification = classification_chain.invoke({"query": query})
    
    return {
        "query_type": classification["type"],
        "use_rag": classification["type"] in ["simple_shelter", "simple_guideline"]
    }

def rag_node(state: HybridAgentState):
    """RAGë¡œ ì§ì ‘ ë‹µë³€í•˜ëŠ” ë…¸ë“œ (LCEL ì‚¬ìš©)"""
    last_message = state["messages"][-1]
    query = last_message.content
    query_type = state.get("query_type", "complex")
    
    try:
        # LCEL ì²´ì¸ ì„ íƒ ë° ì‹¤í–‰
        if query_type == "simple_shelter":
            answer = shelter_rag_chain.invoke(query)
        else:  # simple_guideline
            answer = guideline_rag_chain.invoke(query)
        
        return {"messages": [AIMessage(content=answer)]}
    
    except Exception as e:
        return {"messages": [AIMessage(content=f"RAG ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")]}

# Agentìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
AGENT_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ì¬ë‚œ ì•ˆì „ ë„ìš°ë¯¸ AIì…ë‹ˆë‹¤.

**ì¤‘ìš”í•œ ê·œì¹™**:
1. í˜„ì¬ ì§ˆë¬¸ì—ë§Œ ì§‘ì¤‘í•˜ì„¸ìš”. ì´ì „ ëŒ€í™”ì™€ ë¬´ê´€í•œ ìƒˆë¡œìš´ ì§ˆë¬¸ì´ë©´ ì™„ì „íˆ ë‹¤ë¥¸ ë‹µë³€ì„ í•˜ì„¸ìš”.
2. ì œê³µëœ ë„êµ¬(search_shelter, search_guideline, count_shelters_by_capacity, get_shelter_statistics)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ìœ¼ì„¸ìš”.
3. ë„êµ¬ ê²€ìƒ‰ ê²°ê³¼ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
4. ì§ˆë¬¸ì´ ì¬ë‚œ/ëŒ€í”¼ì†Œì™€ ë¬´ê´€í•˜ë©´ "ì£„ì†¡í•˜ì§€ë§Œ ì¬ë‚œ ì•ˆì „ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì—ë§Œ ë‹µë³€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”.

**ë„êµ¬ ì‚¬ìš© ê°€ì´ë“œ**:
- íŠ¹ì • ì§€ì—­ ëŒ€í”¼ì†Œ ê²€ìƒ‰ â†’ search_shelter
- ì¬ë‚œ í–‰ë™ìš”ë ¹ â†’ search_guideline  
- ìˆ˜ìš©ì¸ì› ê¸°ì¤€ í†µê³„ â†’ count_shelters_by_capacity
- ì „ì²´ ëŒ€í”¼ì†Œ í†µê³„ â†’ get_shelter_statistics
"""

def agent_node(state: HybridAgentState):
    """Agentê°€ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ë…¸ë“œ"""
    messages = state["messages"]
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìœ¼ë©´ ì¶”ê°€
    if not any(isinstance(m, SystemMessage) for m in messages):
        messages = [SystemMessage(content=AGENT_SYSTEM_PROMPT)] + messages
    
    return {"messages": [llm_with_tools.invoke(messages)]}

def route_after_classification(state: HybridAgentState) -> str:
    """ë¶„ë¥˜ í›„ ë¼ìš°íŒ…"""
    if state.get("use_rag", False):
        return "rag"
    else:
        return "agent"

# 8. í•˜ì´ë¸Œë¦¬ë“œ ê·¸ë˜í”„ êµ¬ì„±

memory = MemorySaver()

workflow = StateGraph(HybridAgentState)

workflow.add_node("classifier", classifier_node)
workflow.add_node("rag", rag_node)
workflow.add_node("agent", agent_node)
workflow.add_node("tools", ToolNode(tools))

workflow.add_edge(START, "classifier")
workflow.add_conditional_edges(
    "classifier",
    route_after_classification,
    {"rag": "rag", "agent": "agent"}
)
workflow.add_edge("rag", END)
workflow.add_conditional_edges("agent", tools_condition)
workflow.add_edge("tools", "agent")

app = workflow.compile(checkpointer=memory)

# 9. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤

class ChatSession:
    def __init__(self, session_id: str = "default"):
        self.session_id = session_id
        self.config = {"configurable": {"thread_id": session_id}}
    
    def chat(self, user_input: str, verbose: bool = False, stream: bool = False):
        """ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬"""
        print(f"\nğŸ‘¤ ì‚¬ìš©ì: {user_input}")
        
        try:
            messages = [HumanMessage(content=user_input)]
            
            if stream:
                # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
                print("ğŸ¤– ì±—ë´‡: ", end="", flush=True)
                for chunk in app.stream(
                    {"messages": messages, "use_rag": False, "query_type": None},
                    config=self.config,
                    stream_mode="values"
                ):
                    if "messages" in chunk and chunk["messages"]:
                        last_msg = chunk["messages"][-1]
                        if isinstance(last_msg, AIMessage) and last_msg.content:
                            print(last_msg.content, end="", flush=True)
                print()
            else:
                # ì¼ë°˜ ëª¨ë“œ
                result = app.invoke(
                    {"messages": messages, "use_rag": False, "query_type": None},
                    config=self.config
                )
                
                # ì²˜ë¦¬ ê²½ë¡œ í‘œì‹œ
                if verbose and "query_type" in result:
                    route = "ğŸ” RAG (LCEL)" if result.get("use_rag") else "ğŸ¤– Agent+Tools"
                    print(f"[ê²½ë¡œ: {route} | ìœ í˜•: {result['query_type']}]")
                
                bot_response = result["messages"][-1].content
                print(f"ğŸ¤– ì±—ë´‡:\n{bot_response}")
        
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
    
    def clear_history(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        print("ğŸ”„ ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

def interactive_chat():
    """ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤"""
    session = ChatSession()
    print("=" * 60)
    print("ğŸš¨ ì¬ë‚œ ì•ˆì „ ë„ìš°ë¯¸ ì±—ë´‡ (Hybrid RAG + Agent with LCEL)")
    print("=" * 60)
    print("ëª…ë ¹ì–´: '/exit', '/clear', '/verbose', '/stream'")
    print()
    
    verbose = False
    stream = False
    
    while True:
        try:
            user_input = input("ğŸ‘¤ ì§ˆë¬¸: ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() == '/exit':
                print("ğŸ‘‹ ì•ˆë…•íˆ ê°€ì„¸ìš”!")
                break
            elif user_input.lower() == '/clear':
                session.clear_history()
                continue
            elif user_input.lower() == '/verbose':
                verbose = not verbose
                print(f"ğŸ”„ Verbose ëª¨ë“œ: {'ON' if verbose else 'OFF'}")
                continue
            elif user_input.lower() == '/stream':
                stream = not stream
                print(f"ğŸ”„ Stream ëª¨ë“œ: {'ON' if stream else 'OFF'}")
                continue
            
            session.chat(user_input, verbose=verbose, stream=stream)
        
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ì•ˆë…•íˆ ê°€ì„¸ìš”!")
            break

if __name__ == "__main__":
    interactive_chat()